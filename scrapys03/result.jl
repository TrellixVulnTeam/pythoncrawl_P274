{"contents": "As the COVID-19 pandemic took hold, we at Scrapinghub began to wonder how it would impact on the data we crawl, and whether that data could tell us something useful about the pandemic and its impact.Retail price intelligence is one of our key areas of interest. On a daily basis,  for price and stock level information. What insights might be hidden in that data?To explore this, we identified a basket of goods related to pandemic preparedness (eg Face Masks, medications, etc) and began to track the number of individual items available online in this set, and the average item price over time.In the first chart, you can see the overall price (in blue), and the number of individual items (SKU\u2019s, in retail parlance, in red) over time for a collection of US data sources. You can see there is a lot of variation in the number of items on supply as new vendors enter the market with basic items. As you might expect, the price often moves in the opposite direction, as in the dip around April 5th. Basic economics works, but the average price doesn\u2019t drop away as strongly as you might expect. Underlying demand is clearly strong. Note that we aren\u2019t looking at stock levels here, just the range of items in stock and available for purchase. Variation in the number of items available could be due to opportunistic vendors entering the market, or rebranding routine supplies with COVID-19 related keywords.The strength of the underlying demand is such that the overall average price on the basket increased over 40% from March 29th (when we began to track the series) onwards over two months to May 28th. COVID-19 case counts in the US began to lift off in Mid-March, and with  leading that we would expect the \u2018normal\u2019 basket price had already risen considerably from its pre-COVID-19 base when we began to track the data.\u00a0The interactive map shows the % increase is the average item cost between 1st April and 1st June 2020. You can see that even in a homogeneous market like the US, there is considerable state by state variation in the amount of price increases, with prices more than doubling in some states (Alabama, Wyoming) but showing very modest increases in others (Minnesota). The increases do not seem to correlate with actual observed COVID-19 case levels.The second chart shows the price, overall, and 3 US states, for comparison. You can see the price \u2018wobbles\u2019 quite a lot, often with different states not quite in sync. We expected to see lower prices in states with less active cases, but that doesn\u2019t seem to be the case at all, as the fluid US internal market smooths out price variation within a few days, and thus no systematic variation in price."}
{"contents": "The Internet offers a in the form of , news, blog posts, stories, essays, tutorials that can be leveraged by many useful applications:But anyone interested in using all this data available, will face some challenges.\u00a0Web pages are built of many components (menus, sidebars, ads, etc) and only a few of them represent the true article content, the actual valuable information. Being able to . Especially when you want to obtain this information from a diverse set of sources that can have a different structure, styling and even be in different languages.This challenge is not minor as it is very common to find irrelevant content not only outside of the article body but also within the body itself: elements like ads, links to content not directly related with the article, call to action boxes, social media buttons, etc are very common nowadays.\u00a0Articles content  like images, figure captions, videos, tables, quotes, tweets, etc. A lot of meaning is lost if we only focus on plain text. Converting the content to some standardized and simplified format that is independent of the source page is required to leverage all this rich content. Having such a standard format would open the door to apply the same styling rules for any content, independently of the source. What is more, it would provide the flexibility to enable/disable particular components of the articles or even rearrange them. But converting the diverse content into this format is a .\u00a0"}
{"contents": "Scrapinghub is a fully distributed organization with a remote workforce spread across the globe. This structure will enable us to continue to operate at full capacity during the Coronavirus pandemic and deliver full service to our customers.Businesses all over the world are trying to adapt to the new circumstances brought on by the Coronavirus such as being forced to implement a remote working environment while retaining productivity, a huge challenge for companies that normally don\u2019t work remotely.We have had a lot of queries from our customers, who are doing internal global risk assessments on their supply chains being affected by COVID-19 so want to share our continued commitment to providing our customers with ongoing services during this time while ensuring a safe environment for all of our employees.Our internal risk models predict no more than 2% leave as a worst-case scenario, only slightly above baseline. This is likely to be offset by people deferring normal vacation leave due to travel limitations.Scrapinghub is at exceptionally low risk from any disruption in supplying our customers from the pandemic for two reasons:As a remote-first workplace, our teams are fully distributed. We don\u2019t share office spaces and most of our teams work from home-office environments. With no exposure to co-workers or public transport, the infection risk is greatly reduced. This sort of remote working and social distancing practices other companies are rushing to implement have been business as usual for us since day-1. We do use some co-working spaces from time to time but have taken a step back from those in regions where COVID-19 is community spread. We occasionally have get-togethers but none are planned for the duration of the current pandemic.As a globally distributed workforce, our risk is also reduced. Our workforce is spread across 28 countries, chiefly Europe, Asia, and South America. While some of the team in China are coming out on the other side of the Coronavirus disruption, others in South America live in countries that have yet to experience any cases at all. The geographic spread somewhat insulates us, as even a global crisis like COVID-19 impacts different regions in different ways at different times. Some of our developers located in China have already experienced significant local disruption and quarantines with no impact on their work.In terms of our own supply chain, our key input is a data center and cloud-based infrastructure. Businesses of this type are by its nature, relatively well insulated from supply chain disruption. Most of our data center services are located in the EU but we maintain relationships with multiple providers with a global footprint. Also, if the need arose, we could pivot some or all of our infrastructure to different regions.We would like to offer support to our customers looking for advice on remote working or managing teams remotely as COVID-19 continues to disrupt business. Please reach out to your account or project manager for more information. We wish all our customers and partners a safe and uneventful experience of the pandemic."}
{"contents": "When it comes to web scraping at scale, there\u2019s a set of challenges you need to overcome to extract the data. But once you are able to get it, you still have work to do. You need to have a data QA process in place. Data quality becomes especially crucial if you\u2019re extracting high volumes of data from the web regularly and your team\u2019s success depends on the quality of the scraped data.\n\nThis article is the first of a four-part series of how to maximize web scraped data quality. We are going to share with you all the techniques, tricks and technologies we use at Scrapinghub to extract web data from billions of pages every month, while keeping data quality high.The first step is to understand the business requirements of the web scraping project and define clear, testable rules which will help you detect data quality problems. Understanding requirements clearly is essential to move forward and develop the best data quality process.Requirements are often incomplete, ambiguous or vague. Here you can find some general tips for defining good requirements:In order to show an actual example, in this article we are going to work with product data which was extracted from an e-commerce site. Here is a sample of what two typical scraped records are  look like:In addition to these sample records, the business requirements - that are provided to the QA Engineer - are as follows:Can you find some potential problems in the requirements above?"}
{"contents": "I\u2019d like to echo Joel Gasgoine\u2019s sentiments: This is not normal remote working!Like Buffer, we\u2019ve been a remote-first company for almost 10 years and we\u2019re also adjusting to the new normal as a result of COVID-19.Remote teams tend to favour asynchronous written communication. That is to say, we send text messages and don\u2019t expect an immediate response. This allows longer blocks of uninterrupted working time, it allows \u201cbatching\u201d of replies and works better when people have different working hours. Amir Salihefendic makes the case for Asynchronous Communication in his excellent blog post . This is one of the reasons why so many remote workers feel more productive.On the other hand, synchronous communication is more effective when a topic requires a lot of back-and-forths, or if there is time pressure. Don\u2019t be afraid to \u201cjump on a call\u201d to quickly sort things out. Meetings are better for brainstorming or to achieve a consensus, however, this can be more difficult remotely, especially if participants are not used to it.I believe that there is a tradeoff and it\u2019s best to pick the right communication medium for the message. It helps if guidelines are established around this to make it easier to understand and for everyone to agree upon. It\u2019ll also be easier for everyone if you minimise the number of tools, and don\u2019t make too many changes at once.Here are We have experimented with different initiatives to improve informal interactions and make remote working more fun!The most successful initiatives are often run by diverse teams made up of people from across the company. Even if you have to get something started yourself, it\u2019s a good idea to empower others to run with it.Here are some of our current initiatives:"}
{"contents": "When you extract data from the web at scale, quality assurance is an important process to make sure your web extracted data is consistently of high quality. Validation of this data can be complex though. There are many challenges and problems that need to be addressed. In the second part of this series on web data quality assurance, we will cover most common hurdles and pitfalls in data validation and how to deal with them.I still remember my first lesson when I joined the team. My manager shared 3 simple questions to keep in our mind working on data validation:The problems will be listed in their natural appearance in a typical web scraping project.In the previous post, we discussed the importance of clear, testable requirements. Now let's add more details about what else could be challenging at this point.The QA department is responsible for defining good tests, both in terms of and in terms of . Usually questions like the following are asked:There\u2019s a lot of different kinds of data on the internet. How do you deal with unfamiliar language or structure? Validation is tricky enough in one\u2019s own language. But what can you do when you need to compare two items in Japanese, for example? Let check one example:Can you visually spot the differences between old and new data? If you speak Japanese, you probably can. You will recognise these as the numbers 1 to 10. If you don\u2019t speak Japanese, then visual validation is going to be much more difficult.Another example of complex data is a situation where the data is stored as a nested structure in a single column. This usually happens for data like promotions, variants, options or features."}
{"contents": "Today we are delighted to launch a Beta of our newest data extraction API: . With this API you can collect structured data from web pages that contain automotive data such as classified or dealership sites. Using our API, you can get your data without writing site-specific code. If you need automotive/vehicle data, sign up now for a beta version of our Vehicle API.Whether you are interested in car prices, VIN or other car specific details, our Vehicle API can extract that data for you, at scale.With , you can get access to all the publicly visible details and technical information about the vehicle in a structured JSON.Some of the data fields you get in your API:Our Vehicle API is the perfect choice forWithout AutoExtract Vehicle API you would need to write custom site-specific code for each page you want to extract data from. Plus, you would also need to maintain them and handle all the upcoming technical difficulties. With our Vehicle API, you only need to provide page URLs for the API and then everything else is taken care of, like magic.Under the hood, Vehicle API has a machine learning algorithm that finds all the relevant data fields on the page real-time. This algorithm is constantly improved to make sure you get the best data quality possible.Vehicle API works the same way as other AutoExtract APIs:"}
{"contents": "Web scraping projects usually involve data extraction from many websites. The standard approach to tackle this problem is to write some code to navigate and extract the data from each website. However, this approach may not scale so nicely in the long-term, requiring maintenance effort for each website; it also doesn\u2019t scale in the short-term, when we need to start the extraction process in a couple of weeks. Therefore, we need to think of different solutions to tackle these issues.The problem we propose to solve here is related to  that can be available in HTML form or files, such as PDFs. The catch is that this is required for a few hundreds of different domains and we should be able to scale it up and down without much effort.A brief outline of the problem that needs to be solved:In terms of the solution, file downloading is already built-in Scrapy, it\u2019s just a matter of finding the proper URLs to be downloaded. A routine for HTML article extraction is a bit more tricky, so for this one, we\u2019ll go with AutoExtract\u2019s . This way, we can send any URL to this service and get the content back, together with a probability score of the content being an article or not. Performing a crawl based on some set of input URLs isn\u2019t an issue, given that we can load them from some service (AWS S3, for example).Daily incremental crawls are a bit tricky, as it requires us to store some kind of ID about the information we\u2019ve seen so far. The most basic ID on the web is a URL, so we just hash them to get an ID. Last but not least, by building a single crawler that can handle any domain solves one scalability problem but brings another one to the table. For example, when we build a crawler for each domain, we can run them in parallel using some limited computing resources (like 1GB of RAM). However, once we put everything in a single crawler, especially the incremental crawling requirement, it requires more resources. Consequently, it requires some architectural solution to handle this new scalability issue.From the outline above, we can think of three main tasks that need to be performed:By thinking about each of these tasks separately, we can build an  that follows a producer-consumer strategy. Basically, we have a process of finding URLs based on some inputs (producer) and two approaches for data extraction (consumer). This way, we can build these smaller processes to scale arbitrarily with small computing resources and it enables us to scale horizontally if we add or remove domains. An overview of the proposed solution is depicted below."}
{"contents": "We are excited to announce our next . Using this API, you can get access to product reviews in a structured format, without writing site-specific code. You can use the Product Reviews API to extract product reviews from eCommerce sites at scale. Just make a request to the API and receive your data in real-time!In today\u2019s competitive eCommerce world, product reviews provide a great way for online shoppers to determine what products to buy. Hence, monitoring product reviews are important for businesses. Making use of reviews data, you can find insights in the data that can improve your decision making, address feedback, and monitor customer sentiment.But getting access to structured web data is not easy, especially if you don\u2019t have the right tools. With Product Reviews API, we provide a convenient way for you to extract reviews at scale from any site.More info about the fields in the .Whichever your use case is, you can always rely on Product Reviews API to deliver high-quality data.Before our Product Reviews API, you needed to write a site-specific code to extract reviews or other data. Furthermore, you also needed to maintain the code if the website changed its layout or frontend code.With AutoExtract Product Reviews API, you don\u2019t need to write custom code to extract data. Our AI-based tool will automatically find all the data fields you need and extract it from the page. You just need to submit the target page URLs. Then, you will receive your data in a structured JSON format.Product Reviews API works the same way as other AutoExtract APIs:"}
{"contents": "The manual way or the highway...In software testing and QA circles, the topic of whether automated or manual testing is superior remains a hotly debated one. For  QA and validation specifically, they are not mutually exclusive. Indeed, for data, manual QA can inform automated QA, and vice versa. In this post, we\u2019ll give some examples.It is rare that can be adequately validated with automated techniques alone; additional manual inspections are often needed. The optimal blend of manual and automated tests depends on factors including:When considered in isolation, each have their benefits and drawbacks:, when rules are clearly defined and relatively static, this includes things like:, on the other hand, are invaluable for a deeper understanding of suspected data quality problems, particularly for data extracted from dynamic e-commerce websites and marketplaces.\u00a0From a practical point of view, the validation process should start with an understanding of the data and its characteristics. Next, define what rules are needed to validate the data, and automate them. The results of the automation will be warnings and possible false alarms that need to be verified using manual inspection. After the improvement of the rules, the second iteration of automated checks can be executed.Let's suppose we have the task of verifying the extraction coverage and correctness for this website: "}
